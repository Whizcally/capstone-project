{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12698894,"sourceType":"datasetVersion","datasetId":8025474}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Capstone project was submitted in partial fulfillment of the [Google Advanced Data Analytics Certificate](https://www.coursera.org/specializations/google-advanced-data-analytics). \n\n# Project Overview\n\n## About the company\nSalifort Motors is a fictional French-based alternative energy vehicle manufacturer. Its global workforce of over 100,000 employees research, design, construct, validate, and distribute electric, solar, algae, and hydrogen-based vehicles. Salifortâ€™s end-to-end vertical integration model has made it a global leader at the intersection of alternative energy and automobiles\n\n## Business Case\nAnalyze data from a recent employee survey to come up with ideas for how to increase employee retention. Design a model that predicts whether an employee will leave the company based on their  department, number of projects, average monthly hours, and any other data points. \n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.024961Z","iopub.execute_input":"2025-08-08T13:18:03.025290Z","iopub.status.idle":"2025-08-08T13:18:03.031824Z","shell.execute_reply.started":"2025-08-08T13:18:03.025262Z","shell.execute_reply":"2025-08-08T13:18:03.030658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0 = pd.read_csv(\"/kaggle/input/salifort-motors-datasets/Files/HR_capstone_dataset.csv\")\ndf0.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.067056Z","iopub.execute_input":"2025-08-08T13:18:03.067378Z","iopub.status.idle":"2025-08-08T13:18:03.101001Z","shell.execute_reply.started":"2025-08-08T13:18:03.067354Z","shell.execute_reply":"2025-08-08T13:18:03.100101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Exploration (Initial EDA and data cleaning)","metadata":{}},{"cell_type":"code","source":"df0.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.102829Z","iopub.execute_input":"2025-08-08T13:18:03.103196Z","iopub.status.idle":"2025-08-08T13:18:03.117684Z","shell.execute_reply.started":"2025-08-08T13:18:03.103166Z","shell.execute_reply":"2025-08-08T13:18:03.116398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.119047Z","iopub.execute_input":"2025-08-08T13:18:03.119375Z","iopub.status.idle":"2025-08-08T13:18:03.170815Z","shell.execute_reply.started":"2025-08-08T13:18:03.119350Z","shell.execute_reply":"2025-08-08T13:18:03.169982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.172012Z","iopub.execute_input":"2025-08-08T13:18:03.172329Z","iopub.status.idle":"2025-08-08T13:18:03.178036Z","shell.execute_reply.started":"2025-08-08T13:18:03.172298Z","shell.execute_reply":"2025-08-08T13:18:03.177229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0= df0.rename(columns={'average_montly_hours': 'average_monthly_hours', 'time_spend_company': 'tenure', \n                         'Work_accident': 'work_accident', 'Department': 'department'})\n\ndf0.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.180006Z","iopub.execute_input":"2025-08-08T13:18:03.180407Z","iopub.status.idle":"2025-08-08T13:18:03.210009Z","shell.execute_reply.started":"2025-08-08T13:18:03.180369Z","shell.execute_reply":"2025-08-08T13:18:03.209127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.211171Z","iopub.execute_input":"2025-08-08T13:18:03.211497Z","iopub.status.idle":"2025-08-08T13:18:03.232895Z","shell.execute_reply.started":"2025-08-08T13:18:03.211473Z","shell.execute_reply":"2025-08-08T13:18:03.231931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.233588Z","iopub.execute_input":"2025-08-08T13:18:03.234021Z","iopub.status.idle":"2025-08-08T13:18:03.260223Z","shell.execute_reply.started":"2025-08-08T13:18:03.233997Z","shell.execute_reply":"2025-08-08T13:18:03.259276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0[df0.duplicated()].head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.261110Z","iopub.execute_input":"2025-08-08T13:18:03.261393Z","iopub.status.idle":"2025-08-08T13:18:03.287309Z","shell.execute_reply.started":"2025-08-08T13:18:03.261372Z","shell.execute_reply":"2025-08-08T13:18:03.286364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1= df0.drop_duplicates(keep='first')\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.288368Z","iopub.execute_input":"2025-08-08T13:18:03.288695Z","iopub.status.idle":"2025-08-08T13:18:03.308127Z","shell.execute_reply.started":"2025-08-08T13:18:03.288668Z","shell.execute_reply":"2025-08-08T13:18:03.307150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('Boxplot to detect outliers for tenure', fontsize=12)\nsns.boxplot(x=df1['tenure'])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.310941Z","iopub.execute_input":"2025-08-08T13:18:03.311596Z","iopub.status.idle":"2025-08-08T13:18:03.445643Z","shell.execute_reply.started":"2025-08-08T13:18:03.311566Z","shell.execute_reply":"2025-08-08T13:18:03.444487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determine the number of rows containing outliers\n\npercentile25 = df1['tenure'].quantile(0.25)\n\npercentile75 = df1['tenure'].quantile(0.75)\n\niqr = percentile75 - percentile25\n\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr\nprint(\"Lower limit:\", lower_limit)\nprint(\"Upper limit:\", upper_limit)\n\noutliers = df1[(df1['tenure'] > upper_limit) | (df1['tenure'] < lower_limit)]\n\nprint(\"Number of rows in the data containing outliers in `tenure`:\", len(outliers))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.446495Z","iopub.execute_input":"2025-08-08T13:18:03.446789Z","iopub.status.idle":"2025-08-08T13:18:03.458742Z","shell.execute_reply.started":"2025-08-08T13:18:03.446759Z","shell.execute_reply":"2025-08-08T13:18:03.457522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this initial EDA, I got to know the data and did some basic data cleaning. There were no missing values in the data.\n\nHowever, some of the rows were duplicated. Are these legitimate entries, one might wonder? I could perform a likelihood analysis by essentially applying Bayes' theorem and multiplying the probabilities of finding each value in each column, but this does not seem necessary. With several continuous values across the first 10 columns, it doesn't seem like a legitimate entry, so I proceeded by dropping them. The duplicated rows account for approximately 20% of the data.\n\nUpon checking the distribution of data for the variable 'tenure', there are 824 rows of data outliers. I wouldn't want to do anything to them since they are not much, and I intend to use Random Forest classification models, which handle a certain degree of outliers. ","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration (Analyzing Relationships between Variables)","metadata":{}},{"cell_type":"code","source":"print(df1['left'].value_counts())\nprint(df1['left'].value_counts(normalize=True)*100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.459792Z","iopub.execute_input":"2025-08-08T13:18:03.460131Z","iopub.status.idle":"2025-08-08T13:18:03.484985Z","shell.execute_reply.started":"2025-08-08T13:18:03.460097Z","shell.execute_reply":"2025-08-08T13:18:03.483999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.486143Z","iopub.execute_input":"2025-08-08T13:18:03.486469Z","iopub.status.idle":"2025-08-08T13:18:03.502072Z","shell.execute_reply.started":"2025-08-08T13:18:03.486447Z","shell.execute_reply":"2025-08-08T13:18:03.501079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# boxplot showing `average_monthly_hours` distributions for `number_project`, comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Monthly hours by number of projects', fontsize='14')\n\n#  histogram showing distribution of `number_project`, comparing employees who stayed versus those who left\ntenure_stay = df1[df1['left']==0]['number_project']\ntenure_left = df1[df1['left']==1]['number_project']\nsns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\nax[1].set_title('Number of projects histogram', fontsize='14')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:03.502987Z","iopub.execute_input":"2025-08-08T13:18:03.503257Z","iopub.status.idle":"2025-08-08T13:18:04.142498Z","shell.execute_reply.started":"2025-08-08T13:18:03.503236Z","shell.execute_reply":"2025-08-08T13:18:04.141527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the plot above,\n\n- Everyone with seven projects left the company. They worked the most hours with those who had six projects and also left, with some working nearly 300 hours a month.\n\n- People who left the company can be divided into two groups. The first group worked less, while the second group worked more on the same number of projects. The first group might include people who got fired or possibly those who were assigned smaller tasks because they are already on their way out. It might be a valid reason to say that the second group quit, owing to the stress of having too many projects and working too many hours.\n\n- People who had 3 or 4 projects not only have optimal working hours, but also have a lower left/stayed ratio.\n\n- Almost everyone in the company is overworked if you consider the average hours they are supposed to work in a month. Assuming there are _52 weeks in a year and 2 weeks are for vacations, then we expect 50 weeks * 40 hours per week / 12 months = 166.67 hours per month_.\n\nIt would be wise to get the number of people who had 7 projects, since all of them left. \n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient=\"h\", ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Satisfaction by tenure', fontsize='14')\n\n# histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\ntenure_stay = df1[df1['left']==0]['tenure']\ntenure_left = df1[df1['left']==1]['tenure']\nsns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=5, ax=ax[1])\nax[1].set_title('Tenure histogram', fontsize='14')\n\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:04.143639Z","iopub.execute_input":"2025-08-08T13:18:04.144033Z","iopub.status.idle":"2025-08-08T13:18:04.956672Z","shell.execute_reply.started":"2025-08-08T13:18:04.143985Z","shell.execute_reply":"2025-08-08T13:18:04.955515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From this plot, we can see that \n\n- Again, two groups of people left: dissatisfied employees with shorter tenures and highly satisfied employees with medium-length tenures.\n  \n- There is an unusual satisfaction level data distribution for those four-year employers who left. It would be wise to investigate by checking with the company if possible.\n\n- The satisfaction level of high-tenured and newer employees who stayed is relatively high. ","metadata":{}},{"cell_type":"code","source":"df1.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:04.957648Z","iopub.execute_input":"2025-08-08T13:18:04.958045Z","iopub.status.idle":"2025-08-08T13:18:04.970889Z","shell.execute_reply.started":"2025-08-08T13:18:04.958013Z","shell.execute_reply":"2025-08-08T13:18:04.969946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, people who stayed have higher average and median satisfaction levels compared to those who left. It is also important to note that the average satisfaction level of those who stayed is slightly lower than the median, suggesting that the satisfaction levels of those who stayed are skewed to the left. ","metadata":{}},{"cell_type":"code","source":"#plot to examine relationship between `average_monthly_hours` and `promotion_last_5years`\nplt.figure(figsize=(16, 3))\nsns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.4)\nplt.axvline(x=166.67, color='#ff6361', ls='--')\nplt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\nplt.title('Monthly hours by promotion last 5 years', fontsize='14');","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:04.971753Z","iopub.execute_input":"2025-08-08T13:18:04.972089Z","iopub.status.idle":"2025-08-08T13:18:06.001137Z","shell.execute_reply.started":"2025-08-08T13:18:04.972058Z","shell.execute_reply":"2025-08-08T13:18:06.000177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the above plot,\n\n- Only a few employees were promoted, and very few employers who worked long hours were promoted in the last 5 years.\n\n- Employees who left worked the longest hours. ","metadata":{}},{"cell_type":"code","source":"department_pct = (\n    df1.groupby(['department', 'left']).size()\n    .groupby(level=0).apply(lambda x: x / x.sum())\n    .unstack(fill_value=0)\n)\n\ndepartment_pct.plot(kind='bar', stacked=True, figsize=(8, 5))\nplt.title('Percentage of Stayed/Left by Department', fontsize='10')\nplt.ylabel('Proportion of Employees')\nplt.xlabel('Department')\nplt.xticks(rotation=75)\nplt.legend(title='Left Company', labels=['Stayed', 'Left'])\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:06.002412Z","iopub.execute_input":"2025-08-08T13:18:06.003264Z","iopub.status.idle":"2025-08-08T13:18:06.306631Z","shell.execute_reply.started":"2025-08-08T13:18:06.003228Z","shell.execute_reply":"2025-08-08T13:18:06.305755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is a uniform proportion of those who left/stayed across all departments. ","metadata":{}},{"cell_type":"code","source":"palette = {0: \"#1f77b4\", 1: \"#ff7f0e\"}  \ng = sns.FacetGrid(df1, col=\"left\", hue=\"left\", palette=palette, height=5)\ng.map_dataframe(sns.scatterplot, x='average_monthly_hours', y='last_evaluation', alpha=0.5)\ng.set_axis_labels(\"Average Monthly Hours\", \"Last Evaluation Score\")\ng.add_legend(title=\"Left the Company\")\ng.fig.suptitle('Monthly Hours vs Evaluation: Stayed vs Left', y=1.05)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:06.307518Z","iopub.execute_input":"2025-08-08T13:18:06.307866Z","iopub.status.idle":"2025-08-08T13:18:07.213312Z","shell.execute_reply.started":"2025-08-08T13:18:06.307843Z","shell.execute_reply":"2025-08-08T13:18:07.212394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the plot,\n\n- The two groups of people who left show that one group was overworked but had high evaluation scores, while the other group worked slightly below the normal average hours but received poor evaluation scores.\n\n- Hours worked and the evaluation score seem to correlate.\n\n- Most of the employees worked more than 167 hours per month.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 9))\nheatmap = sns.heatmap(df1.drop(['department', 'salary'], axis=1).corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:07.214665Z","iopub.execute_input":"2025-08-08T13:18:07.215265Z","iopub.status.idle":"2025-08-08T13:18:07.938669Z","shell.execute_reply.started":"2025-08-08T13:18:07.215233Z","shell.execute_reply":"2025-08-08T13:18:07.937441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The correlation heatmap indicates that the monthly hours, the number of projects worked, and the satisfaction levels all exhibit a positive correlation. The tendency of an employee to leave is negatively correlated with their satisfaction levels.\n\nA major insight from this EDA is that management is clearly poor. Many employees left because of longer working hours, multiple projects, and generally low satisfaction scores. It is also unfavorable to work these many hours and receive no promotions and low evaluation scores. Finally, those who spent more than six years are less likely to leave. ","metadata":{}},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"There are many classification models available, but I prefer to use the Random Forest classification model because of its robustness and performance, as I mentioned earlier. Since there is a class imbalance in the dependent variable 'left', my priority is to tune the model to identify as many people who leave as possible. To focus on minimizing false negatives, I will use recall as the main evaluation metric when tuning and assessing the model.","metadata":{}},{"cell_type":"markdown","source":"To begin, we encode the categorical variables. ","metadata":{}},{"cell_type":"code","source":"df_model = df1.copy()\n\ndf_model['salary'] = (\n    df_model['salary'].astype('category')\n    .cat.set_categories(['low', 'medium', 'high'])\n    .cat.codes\n)\n\ndf_model = pd.get_dummies(df_model, drop_first=True)\n\ndf_model.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:07.939951Z","iopub.execute_input":"2025-08-08T13:18:07.940237Z","iopub.status.idle":"2025-08-08T13:18:07.964414Z","shell.execute_reply.started":"2025-08-08T13:18:07.940216Z","shell.execute_reply":"2025-08-08T13:18:07.963555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we split the data into three parts: training, validation, and test sets.","metadata":{}},{"cell_type":"code","source":"X = df_model.drop('left', axis=1)\ny = df_model['left']\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, stratify=y_tr, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:07.968525Z","iopub.execute_input":"2025-08-08T13:18:07.968912Z","iopub.status.idle":"2025-08-08T13:18:07.991829Z","shell.execute_reply.started":"2025-08-08T13:18:07.968883Z","shell.execute_reply":"2025-08-08T13:18:07.990816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x in [X_train, X_val, X_test]:\n    print(len(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:07.992824Z","iopub.execute_input":"2025-08-08T13:18:07.993141Z","iopub.status.idle":"2025-08-08T13:18:07.998833Z","shell.execute_reply.started":"2025-08-08T13:18:07.993114Z","shell.execute_reply":"2025-08-08T13:18:07.997939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We continue by instantiating the model and running a grid search for hyperparameter tuning.  ","metadata":{}},{"cell_type":"code","source":"# Instantiate the random forest classifier\nrf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\n# dictionary of hyperparameters to tune\ncv_params = {'max_depth': [None],\n             'max_features': [1.0],\n             'min_samples_leaf': [2],\n             'min_samples_split': [2],\n             'n_estimators': [100, 200, 300, 400],\n             }\n\n# list of scoring metrics to capture\nscoring = ['accuracy', 'precision', 'recall', 'f1']\n\n# Instantiate the GridSearchCV object\nrf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='recall')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:07.999669Z","iopub.execute_input":"2025-08-08T13:18:07.999987Z","iopub.status.idle":"2025-08-08T13:18:08.017194Z","shell.execute_reply.started":"2025-08-08T13:18:07.999955Z","shell.execute_reply":"2025-08-08T13:18:08.015999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we train the model.","metadata":{}},{"cell_type":"code","source":"%%time\nrf_cv.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:08.018249Z","iopub.execute_input":"2025-08-08T13:18:08.018605Z","iopub.status.idle":"2025-08-08T13:18:54.802165Z","shell.execute_reply.started":"2025-08-08T13:18:08.018576Z","shell.execute_reply":"2025-08-08T13:18:54.801295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_cv.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.803451Z","iopub.execute_input":"2025-08-08T13:18:54.803687Z","iopub.status.idle":"2025-08-08T13:18:54.809932Z","shell.execute_reply.started":"2025-08-08T13:18:54.803669Z","shell.execute_reply":"2025-08-08T13:18:54.808973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_cv.best_score_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.810873Z","iopub.execute_input":"2025-08-08T13:18:54.811174Z","iopub.status.idle":"2025-08-08T13:18:54.825393Z","shell.execute_reply.started":"2025-08-08T13:18:54.811154Z","shell.execute_reply":"2025-08-08T13:18:54.824495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_results(model_name:str, model_object, metric:str):\n    '''\n    Arguments:\n        model_name (string): what you want the model to be called in the output table\n        model_object: a fit GridSearchCV object\n        metric (string): precision, recall, f1, or accuracy\n\n    Returns a pandas df with the F1, recall, precision, and accuracy scores\n    for the model with the best mean 'metric' score across all validation folds.\n    '''\n\n    # dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {'precision': 'mean_test_precision',\n                   'recall': 'mean_test_recall',\n                   'f1': 'mean_test_f1',\n                   'accuracy': 'mean_test_accuracy',\n                   }\n\n    # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n\n    # Isolate the row of the df with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n\n    # Extract accuracy, precision, recall, and f1 score from that row\n    f1 = best_estimator_results.mean_test_f1\n    recall = best_estimator_results.mean_test_recall\n    precision = best_estimator_results.mean_test_precision\n    accuracy = best_estimator_results.mean_test_accuracy\n\n    # Create table of results\n    table = pd.DataFrame({'model': [model_name],\n                          'precision': [precision],\n                          'recall': [recall],\n                          'F1': [f1],\n                          'accuracy': [accuracy]\n                          }\n                         )\n\n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.826372Z","iopub.execute_input":"2025-08-08T13:18:54.826658Z","iopub.status.idle":"2025-08-08T13:18:54.842413Z","shell.execute_reply.started":"2025-08-08T13:18:54.826636Z","shell.execute_reply":"2025-08-08T13:18:54.841242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = make_results('RF cv', rf_cv, 'recall')\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.843566Z","iopub.execute_input":"2025-08-08T13:18:54.844010Z","iopub.status.idle":"2025-08-08T13:18:54.870731Z","shell.execute_reply.started":"2025-08-08T13:18:54.843979Z","shell.execute_reply":"2025-08-08T13:18:54.869658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After training and cross-validation, we have a random forest model with optimum parameters. The mean recall, precision, f1, and accuracy scores across the validation folds are relatively high. \n\nThe next step is to use the best random forest model to predict on the validation data. ","metadata":{}},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"code","source":"rf_val_preds = rf_cv.best_estimator_.predict(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.871528Z","iopub.execute_input":"2025-08-08T13:18:54.871790Z","iopub.status.idle":"2025-08-08T13:18:54.909762Z","shell.execute_reply.started":"2025-08-08T13:18:54.871771Z","shell.execute_reply":"2025-08-08T13:18:54.908964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_test_scores(model_name:str, preds, y_test_data):\n    '''\n    Generate a table of test scores.\n\n    In:\n        model_name (string): Your choice: how the model will be named in the output table\n        preds: numpy array of test predictions\n        y_test_data: numpy array of y_test data\n\n    Out:\n        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n    '''\n    accuracy = accuracy_score(y_test_data, preds)\n    precision = precision_score(y_test_data, preds)\n    recall = recall_score(y_test_data, preds)\n    f1 = f1_score(y_test_data, preds)\n\n    table = pd.DataFrame({'model': [model_name],\n                          'precision': [precision],\n                          'recall': [recall],\n                          'F1': [f1],\n                          'accuracy': [accuracy]\n                          })\n\n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.910582Z","iopub.execute_input":"2025-08-08T13:18:54.910935Z","iopub.status.idle":"2025-08-08T13:18:54.916878Z","shell.execute_reply.started":"2025-08-08T13:18:54.910909Z","shell.execute_reply":"2025-08-08T13:18:54.915986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_val_scores = get_test_scores('RF val', rf_val_preds, y_val)\n\n# Append to the results table\nresults = pd.concat([results, rf_val_scores], axis=0)\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.917800Z","iopub.execute_input":"2025-08-08T13:18:54.918739Z","iopub.status.idle":"2025-08-08T13:18:54.955205Z","shell.execute_reply.started":"2025-08-08T13:18:54.918685Z","shell.execute_reply":"2025-08-08T13:18:54.954522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Beautiful! The model performed well in predicting the validation data. Now, let's see how it will do on the test data. ","metadata":{}},{"cell_type":"code","source":"rf_test_preds = rf_cv.best_estimator_.predict(X_test)\n\nrf_test_scores = get_test_scores('RF test', rf_test_preds, y_test)\n\nresults = pd.concat([results, rf_test_scores], axis=0)\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:18:54.956142Z","iopub.execute_input":"2025-08-08T13:18:54.956477Z","iopub.status.idle":"2025-08-08T13:18:55.009314Z","shell.execute_reply.started":"2025-08-08T13:18:54.956447Z","shell.execute_reply":"2025-08-08T13:18:55.008588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test, rf_test_preds, labels=rf_cv.classes_)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                             display_labels=['active', 'left'])\ndisp.plot();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:22:24.773374Z","iopub.execute_input":"2025-08-08T13:22:24.774198Z","iopub.status.idle":"2025-08-08T13:22:24.974181Z","shell.execute_reply.started":"2025-08-08T13:22:24.774166Z","shell.execute_reply":"2025-08-08T13:22:24.973266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model predicts more false negatives than false positives, meaning that some employees who are genuinely at risk of leaving or being fired are not identified by the model. However, the number of missed cases remains relatively small, showing that the model is still effective at capturing most at-risk employees.\n\nIt would be helpful to analyze the variables in the data that contributed to the predictions outcomes. ","metadata":{}},{"cell_type":"code","source":"# Get feature importances\nfeat_impt = rf_cv.best_estimator_.feature_importances_\n\n# Get indices of top 10 features\nind = np.argpartition(rf_cv.best_estimator_.feature_importances_, -10)[-10:]\n\n# Get column labels of top 10 features \nfeat = X.columns[ind]\n\n# Filter `feat_impt` to consist of top 10 feature importances\nfeat_impt = feat_impt[ind]\n\ny_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\ny_sort_df = y_df.sort_values(\"Importance\")\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\ny_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\")\n\nax1.set_title(\"Random Forest: Feature Importances for Employee Leaving\", fontsize=12)\nax1.set_ylabel(\"Feature\")\nax1.set_xlabel(\"Importance\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:58:07.949245Z","iopub.execute_input":"2025-08-08T13:58:07.949663Z","iopub.status.idle":"2025-08-08T13:58:08.200825Z","shell.execute_reply.started":"2025-08-08T13:58:07.949635Z","shell.execute_reply":"2025-08-08T13:58:08.199798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the plot above, it is clear that 'satisfaction_level', 'tenure', 'last_evaluation', 'average_monthly_hours', and 'number_project' had the greatest influence on predicting the outcome, respectively. ","metadata":{}},{"cell_type":"markdown","source":"# Results and Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Summary of Model Results","metadata":{}},{"cell_type":"markdown","source":"The Random Forest model achieved a precision of 97.6%, a recall of 92.7%, an F1 score of 95.1%, and an accuracy of 98.4% on the test set.","metadata":{}},{"cell_type":"markdown","source":"## Conclusions and Recommendations","metadata":{}},{"cell_type":"markdown","source":"As pointed out in the EDA and also in the models' feature importance plot, the employees are overworked. \n\nTo support employee retention, the following recommendations are proposed:\n\n- Limit the number of projects assigned to each employee to help manage workload and reduce burnout.\n\n- Review the experiences of employees who have been with the company for four years or more. Consider promoting them where appropriate or investigating potential reasons for dissatisfaction within this group.\n\n- Create a fair approach to longer working hours. Either recognize and reward extended efforts or avoid making them a requirement.\n\n- Ensure all employees clearly understand overtime policies and expectations around working hours and time off. Communicate these policies openly and make sure they are consistently applied.\n\n- Encourage open conversations about the company culture. Hold discussions at both the company and team levels to understand concerns and identify areas for improvement.\n\n- Avoid tying high evaluation scores only to employees who work excessively long hours. Use a performance recognition system that rewards effort, contribution, and quality of work, not just time spent.","metadata":{}}]}